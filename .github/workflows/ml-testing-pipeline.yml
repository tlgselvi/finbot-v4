name: ML Analytics Testing Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/ml/**'
      - 'src/analytics/**'
      - 'tests/**'
      - 'infrastructure/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/ml/**'
      - 'src/analytics/**'
      - 'tests/**'
      - 'infrastructure/**'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: finbot/ml-analytics

jobs:
  # Code Quality and Linting
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy pylint bandit safety
        pip install -r requirements.txt
        
    - name: Run Black (code formatting)
      run: black --check --diff src/ tests/
      
    - name: Run isort (import sorting)
      run: isort --check-only --diff src/ tests/
      
    - name: Run flake8 (linting)
      run: flake8 src/ tests/ --max-line-length=88 --extend-ignore=E203,W503
      
    - name: Run mypy (type checking)
      run: mypy src/ --ignore-missing-imports
      
    - name: Run pylint (code analysis)
      run: pylint src/ --disable=C0114,C0115,C0116
      
    - name: Run bandit (security analysis)
      run: bandit -r src/ -f json -o bandit-report.json
      
    - name: Run safety (dependency security check)
      run: safety check --json --output safety-report.json
      
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    needs: code-quality
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10']
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_ml_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:6
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        
    - name: Run unit tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_ml_db
        REDIS_URL: redis://localhost:6379
        ENVIRONMENT: test
      run: |
        pytest tests/unit/ \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --cov-fail-under=80 \
          --junitxml=junit.xml \
          -v
          
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          junit.xml
          htmlcov/

  # ML Model Tests
  ml-model-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install ML dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-ml.txt
        
    - name: Download test data
      run: |
        mkdir -p data/test
        # Download or generate test datasets
        python scripts/generate_test_data.py
        
    - name: Test model training
      run: |
        pytest tests/ml/test_model_training.py -v --tb=short
        
    - name: Test model inference
      run: |
        pytest tests/ml/test_model_inference.py -v --tb=short
        
    - name: Test model validation
      run: |
        pytest tests/ml/test_model_validation.py -v --tb=short
        
    - name: Model performance benchmarks
      run: |
        python tests/ml/benchmark_models.py --output benchmarks.json
        
    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: model-test-artifacts
        path: |
          models/test/
          benchmarks.json

  # Integration Tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, ml-model-tests]
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_ml_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:6
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        npm install
        
    - name: Start ML services
      run: |
        docker-compose -f docker-compose.test.yml up -d
        sleep 30  # Wait for services to start
        
    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_ml_db
        REDIS_URL: redis://localhost:6379
        ML_API_URL: http://localhost:8080
        ENVIRONMENT: test
      run: |
        pytest tests/integration/ \
          --tb=short \
          --maxfail=5 \
          -v
          
    - name: Stop services
      if: always()
      run: |
        docker-compose -f docker-compose.test.yml down

  # End-to-End Tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        npm install
        npm install -g @playwright/test
        playwright install
        
    - name: Start full application stack
      run: |
        docker-compose -f docker-compose.e2e.yml up -d
        sleep 60  # Wait for all services to start
        
    - name: Run E2E tests
      run: |
        pytest tests/e2e/ \
          --tb=short \
          --maxfail=3 \
          -v
          
    - name: Run Playwright tests
      run: |
        npx playwright test tests/e2e/playwright/
        
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          test-results/
          playwright-report/
          
    - name: Stop services
      if: always()
      run: |
        docker-compose -f docker-compose.e2e.yml down

  # Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust
        
    - name: Start ML services
      run: |
        docker-compose -f docker-compose.test.yml up -d
        sleep 30
        
    - name: Run performance tests
      run: |
        locust -f tests/performance/locustfile.py \
          --host http://localhost:8080 \
          --users 100 \
          --spawn-rate 10 \
          --run-time 5m \
          --html performance-report.html \
          --headless
          
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance-report.html
        
    - name: Stop services
      if: always()
      run: |
        docker-compose -f docker-compose.test.yml down

  # Security Tests
  security-tests:
    runs-on: ubuntu-latest
    needs: code-quality
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
        
    - name: Run OWASP ZAP security scan
      run: |
        docker run -v $(pwd):/zap/wrk/:rw \
          -t owasp/zap2docker-stable zap-baseline.py \
          -t http://localhost:8080 \
          -J zap-report.json \
          -r zap-report.html
          
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: |
          zap-report.json
          zap-report.html

  # Build and Push Docker Images
  build-and-push:
    runs-on: ubuntu-latest
    needs: [unit-tests, ml-model-tests, integration-tests]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
      
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.DOCKER_REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
          
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        file: ./Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  # Deploy to Staging
  deploy-staging:
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG_STAGING }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig
        
    - name: Deploy to staging
      run: |
        export KUBECONFIG=kubeconfig
        helm upgrade --install finbot-ml-staging \
          ./infrastructure/helm/finbot-ml \
          --namespace finbot-ml-staging \
          --create-namespace \
          --set global.environment=staging \
          --set image.tag=${{ github.sha }} \
          --wait --timeout=10m
          
    - name: Run staging smoke tests
      run: |
        export KUBECONFIG=kubeconfig
        kubectl wait --for=condition=ready pod \
          -l app=ml-analytics \
          -n finbot-ml-staging \
          --timeout=300s
        pytest tests/smoke/ --staging-url=https://staging-ml-api.finbot.com

  # Notify on completion
  notify:
    runs-on: ubuntu-latest
    needs: [e2e-tests, performance-tests, security-tests, deploy-staging]
    if: always()
    
    steps:
    - name: Notify Slack
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#ml-team'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        fields: repo,message,commit,author,action,eventName,ref,workflow