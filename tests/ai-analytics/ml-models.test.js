/**
 * AI Financial Analytics - ML Models Tests
 * 
 * Comprehensive test suite for machine learning models including:
 * - Spending prediction model testing
 * - Anomaly detection model testing  
 * - Risk assessment model testing
 * - Model performance validation
 * - Bias detection and fairness testing
 * 
 * Requirements: 1.1, 3.1, 4.1
 */

const { describe, test, expect, beforeEach, afterEach, jest } = require('@jest/globals');
const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs').promises;

// Mock Python execution for testing
const mockPythonExecution = (scriptPath, args = []) => {
    return new Promise((resolve, reject) => {
        // Mock successful execution
        const mockResults = {
            'spending-prediction-model.py': {
                status: 'success',
                predictions: [100, 120, 95, 110, 130, 105, 115],
                metrics: {
                    mae: 15.2,
                    mse: 342.1,
                    rmse: 18.5,
                    r2: 0.85,
                    mape: 12.3
                },
                model_info: {
                    model_version: '1.0',
                    is_trained: true,
                    has_lstm: true,
                    has_ensemble: true
                }
            },
            'anomaly-detection-model.py': {
                status: 'success',
                anomalies_detected: 5,
                anomaly_rate: 0.05,
                predictions: [
                    { timestamp: '2024-01-01T10:00:00', is_anomaly: false, anomaly_score: 0.2 },
                    { timestamp: '2024-01-01T14:30:00', is_anomaly: true, anomaly_score: 0.8 }
                ],
                model_agreement: {
                    isolation_forest_autoencoder: 0.92,
                    all_models: 0.87
                }
            },
            'risk-assessment-model.py': {
                status: 'success',
                assessment: {
                    credit_risk: {
                        risk_probability: 0.15,
                        risk_category: 'low',
                        confidence: 0.89
                    },
                    portfolio_risk: {
                        value_at_risk: 0.08,
                        risk_category: 'medium'
                    },
                    emergency_fund: {
                        recommended_amount: 18000,
                        adequacy_ratio: 0.67
                    }
                }
            }
        };

        const scriptName = path.basename(scriptPath);
        const result = mockResults[scriptName] || { status: 'error', message: 'Unknown script' };
        
        setTimeout(() => resolve(JSON.stringify(result)), 100);
    });
};

describe('ML Models Integration Tests', () => {
    let originalSpawn;

    beforeEach(() => {
        // Mock child_process.spawn for Python execution
        originalSpawn = spawn;
        jest.spyOn(require('child_process'), 'spawn').mockImplementation((command, args) => {
            const mockProcess = {
                stdout: {
                    on: jest.fn((event, callback) => {
                        if (event === 'data') {
                            // Simulate Python script output
                            setTimeout(() => callback(Buffer.from('{"status": "success"}')), 50);
                        }
                    })
                },
                stderr: {
                    on: jest.fn()
                },
                on: jest.fn((event, callback) => {
                    if (event === 'close') {
                        setTimeout(() => callback(0), 100);
                    }
                })
            };
            return mockProcess;
        });
    });

    afterEach(() => {
        jest.restoreAllMocks();
    });

    describe('Spending Prediction Model Tests', () => {
        test('should train spending prediction model successfully', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['train']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.model_info.is_trained).toBe(true);
            expect(data.model_info.has_lstm).toBe(true);
            expect(data.model_info.has_ensemble).toBe(true);
        });

        test('should make accurate spending predictions', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['predict']);
            const data = JSON.parse(result);

            expect(data.predictions).toHaveLength(7);
            expect(data.predictions.every(p => typeof p === 'number')).toBe(true);
            expect(data.predictions.every(p => p > 0)).toBe(true);
        });

        test('should validate model performance metrics', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['evaluate']);
            const data = JSON.parse(result);

            expect(data.metrics.mae).toBeLessThan(50); // Mean Absolute Error < 50
            expect(data.metrics.r2).toBeGreaterThan(0.7); // RÂ² > 0.7
            expect(data.metrics.mape).toBeLessThan(20); // MAPE < 20%
        });

        test('should handle insufficient training data', async () => {
            // Mock insufficient data scenario
            jest.spyOn(require('child_process'), 'spawn').mockImplementation(() => ({
                stdout: { on: jest.fn((event, cb) => cb(Buffer.from('{"status": "error", "message": "Insufficient data"}'))) },
                stderr: { on: jest.fn() },
                on: jest.fn((event, cb) => cb(1))
            }));

            const result = await mockPythonExecution('spending-prediction-model.py', ['train']);
            const data = JSON.parse(result);

            expect(data.status).toBe('error');
            expect(data.message).toContain('Insufficient data');
        });

        test('should detect seasonal patterns in spending', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['analyze_patterns']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            // Seasonal patterns should be detected
            expect(data.predictions).toBeDefined();
        });
    });

    describe('Anomaly Detection Model Tests', () => {
        test('should train anomaly detection model successfully', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['train']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.model_agreement.all_models).toBeGreaterThan(0.8);
        });

        test('should detect spending anomalies accurately', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['detect']);
            const data = JSON.parse(result);

            expect(data.anomaly_rate).toBeLessThan(0.1); // Less than 10% anomalies expected
            expect(data.predictions).toBeInstanceOf(Array);
            expect(data.predictions.every(p => 
                typeof p.is_anomaly === 'boolean' && 
                typeof p.anomaly_score === 'number'
            )).toBe(true);
        });

        test('should maintain high model agreement', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['evaluate']);
            const data = JSON.parse(result);

            expect(data.model_agreement.isolation_forest_autoencoder).toBeGreaterThan(0.85);
            expect(data.model_agreement.all_models).toBeGreaterThan(0.8);
        });

        test('should handle edge cases in anomaly detection', async () => {
            // Test with extreme values
            const result = await mockPythonExecution('anomaly-detection-model.py', ['test_edge_cases']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
        });

        test('should provide anomaly explanations', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['explain']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            // Should provide explanations for detected anomalies
        });
    });

    describe('Risk Assessment Model Tests', () => {
        test('should train risk assessment models successfully', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['train']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
        });

        test('should assess credit risk accurately', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['assess_credit']);
            const data = JSON.parse(result);

            expect(data.assessment.credit_risk.risk_probability).toBeGreaterThanOrEqual(0);
            expect(data.assessment.credit_risk.risk_probability).toBeLessThanOrEqual(1);
            expect(['very_low', 'low', 'medium', 'high', 'very_high'])
                .toContain(data.assessment.credit_risk.risk_category);
            expect(data.assessment.credit_risk.confidence).toBeGreaterThan(0.5);
        });

        test('should calculate portfolio risk correctly', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['assess_portfolio']);
            const data = JSON.parse(result);

            expect(data.assessment.portfolio_risk.value_at_risk).toBeGreaterThan(0);
            expect(data.assessment.portfolio_risk.value_at_risk).toBeLessThan(1);
            expect(['low', 'medium', 'high', 'very_high'])
                .toContain(data.assessment.portfolio_risk.risk_category);
        });

        test('should recommend optimal emergency fund', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['assess_emergency']);
            const data = JSON.parse(result);

            expect(data.assessment.emergency_fund.recommended_amount).toBeGreaterThan(0);
            expect(data.assessment.emergency_fund.adequacy_ratio).toBeGreaterThanOrEqual(0);
        });

        test('should provide comprehensive risk recommendations', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['comprehensive']);
            const data = JSON.parse(result);

            expect(data.assessment).toHaveProperty('credit_risk');
            expect(data.assessment).toHaveProperty('portfolio_risk');
            expect(data.assessment).toHaveProperty('emergency_fund');
        });
    });
});    de
scribe('Model Performance and Validation Tests', () => {
        test('should validate model accuracy benchmarks', async () => {
            const models = ['spending-prediction', 'anomaly-detection', 'risk-assessment'];
            
            for (const model of models) {
                const result = await mockPythonExecution(`${model}-model.py`, ['benchmark']);
                const data = JSON.parse(result);
                
                expect(data.status).toBe('success');
                
                // Performance benchmarks
                if (model === 'spending-prediction') {
                    expect(data.metrics.r2).toBeGreaterThan(0.7);
                    expect(data.metrics.mape).toBeLessThan(25);
                }
                
                if (model === 'anomaly-detection') {
                    expect(data.precision).toBeGreaterThan(0.8);
                    expect(data.recall).toBeGreaterThan(0.7);
                }
                
                if (model === 'risk-assessment') {
                    expect(data.accuracy).toBeGreaterThan(0.8);
                    expect(data.auc_score).toBeGreaterThan(0.85);
                }
            }
        });

        test('should handle model overfitting detection', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['overfitting_check']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            
            // Training vs validation performance gap should be reasonable
            const performanceGap = Math.abs(data.train_score - data.validation_score);
            expect(performanceGap).toBeLessThan(0.1); // Less than 10% gap
        });

        test('should validate model stability across different datasets', async () => {
            const datasets = ['dataset1', 'dataset2', 'dataset3'];
            const results = [];

            for (const dataset of datasets) {
                const result = await mockPythonExecution('spending-prediction-model.py', ['test', dataset]);
                const data = JSON.parse(result);
                results.push(data.metrics.r2);
            }

            // Model performance should be stable across datasets
            const meanR2 = results.reduce((a, b) => a + b) / results.length;
            const variance = results.reduce((acc, r2) => acc + Math.pow(r2 - meanR2, 2), 0) / results.length;
            
            expect(variance).toBeLessThan(0.05); // Low variance indicates stability
        });

        test('should perform cross-validation successfully', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['cross_validate']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.cv_scores).toHaveLength(5); // 5-fold CV
            expect(data.mean_cv_score).toBeGreaterThan(0.75);
            expect(data.cv_std).toBeLessThan(0.1); // Low standard deviation
        });
    });

    describe('Bias Detection and Fairness Tests', () => {
        test('should detect demographic bias in risk assessment', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['bias_test']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            
            // Bias metrics should be within acceptable ranges
            expect(data.demographic_parity).toBeGreaterThan(0.8);
            expect(data.equalized_odds).toBeGreaterThan(0.8);
            expect(data.calibration_score).toBeGreaterThan(0.9);
        });

        test('should ensure fairness across age groups', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['fairness_age']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            
            // Performance should be similar across age groups
            const ageGroups = Object.keys(data.age_group_performance);
            const performances = Object.values(data.age_group_performance);
            
            const maxPerf = Math.max(...performances);
            const minPerf = Math.min(...performances);
            
            expect((maxPerf - minPerf) / maxPerf).toBeLessThan(0.1); // Less than 10% difference
        });

        test('should validate gender fairness in predictions', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['gender_fairness']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.gender_bias_score).toBeLessThan(0.1); // Low bias score
        });

        test('should check for income-based discrimination', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['income_bias']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.income_discrimination_score).toBeLessThan(0.15);
        });
    });

    describe('Model Robustness and Security Tests', () => {
        test('should handle adversarial inputs', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['adversarial_test']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.adversarial_robustness_score).toBeGreaterThan(0.8);
        });

        test('should validate input sanitization', async () => {
            const maliciousInputs = [
                'DROP TABLE users;',
                '<script>alert("xss")</script>',
                '../../etc/passwd',
                'null',
                'undefined'
            ];

            for (const input of maliciousInputs) {
                const result = await mockPythonExecution('spending-prediction-model.py', ['sanitize_test', input]);
                const data = JSON.parse(result);
                
                expect(data.status).toBe('success');
                expect(data.input_sanitized).toBe(true);
            }
        });

        test('should handle extreme values gracefully', async () => {
            const extremeValues = [
                { amount: Number.MAX_SAFE_INTEGER },
                { amount: -Number.MAX_SAFE_INTEGER },
                { amount: 0 },
                { amount: NaN },
                { amount: Infinity }
            ];

            for (const value of extremeValues) {
                const result = await mockPythonExecution('anomaly-detection-model.py', ['extreme_test', JSON.stringify(value)]);
                const data = JSON.parse(result);
                
                expect(data.status).toBe('success');
                expect(data.handled_gracefully).toBe(true);
            }
        });

        test('should validate model versioning and compatibility', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['version_check']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.model_version).toBeDefined();
            expect(data.compatibility_check).toBe(true);
        });
    });

    describe('Performance and Scalability Tests', () => {
        test('should handle large datasets efficiently', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['large_dataset_test']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.processing_time).toBeLessThan(300); // Less than 5 minutes
            expect(data.memory_usage_mb).toBeLessThan(2048); // Less than 2GB
        });

        test('should maintain prediction latency under load', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['latency_test']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.average_prediction_time_ms).toBeLessThan(100); // Less than 100ms
            expect(data.p95_prediction_time_ms).toBeLessThan(200); // 95th percentile < 200ms
        });

        test('should scale with concurrent requests', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['concurrency_test']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.concurrent_requests_handled).toBeGreaterThan(50);
            expect(data.error_rate).toBeLessThan(0.01); // Less than 1% error rate
        });

        test('should optimize memory usage during training', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['memory_optimization_test']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.peak_memory_usage_mb).toBeLessThan(4096); // Less than 4GB
            expect(data.memory_efficiency_score).toBeGreaterThan(0.8);
        });
    });

    describe('Model Interpretability Tests', () => {
        test('should provide feature importance rankings', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['feature_importance']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.feature_importance).toBeDefined();
            expect(Object.keys(data.feature_importance)).toHaveLength.toBeGreaterThan(5);
            
            // Feature importance values should sum to approximately 1
            const totalImportance = Object.values(data.feature_importance).reduce((a, b) => a + b, 0);
            expect(Math.abs(totalImportance - 1)).toBeLessThan(0.1);
        });

        test('should generate SHAP explanations', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['shap_explanation']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.shap_values).toBeDefined();
            expect(data.explanation_quality_score).toBeGreaterThan(0.7);
        });

        test('should provide prediction confidence intervals', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['confidence_intervals']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.predictions).toBeDefined();
            expect(data.confidence_intervals).toBeDefined();
            expect(data.confidence_intervals.lower_bound).toBeDefined();
            expect(data.confidence_intervals.upper_bound).toBeDefined();
        });

        test('should explain anomaly detection decisions', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['explain_decisions']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.explanations).toBeDefined();
            expect(data.explanations.length).toBeGreaterThan(0);
        });
    });

    describe('Model Monitoring and Drift Detection Tests', () => {
        test('should detect data drift in input features', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['data_drift_test']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.drift_detected).toBeDefined();
            expect(data.drift_score).toBeGreaterThanOrEqual(0);
            expect(data.drift_score).toBeLessThanOrEqual(1);
        });

        test('should monitor model performance degradation', async () => {
            const result = await mockPythonExecution('risk-assessment-model.py', ['performance_monitoring']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.performance_trend).toBeDefined();
            expect(data.degradation_alert).toBeDefined();
        });

        test('should track prediction distribution changes', async () => {
            const result = await mockPythonExecution('anomaly-detection-model.py', ['distribution_monitoring']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.distribution_shift_score).toBeGreaterThanOrEqual(0);
            expect(data.statistical_tests).toBeDefined();
        });

        test('should validate model freshness', async () => {
            const result = await mockPythonExecution('spending-prediction-model.py', ['freshness_check']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.model_age_days).toBeLessThan(90); // Model should be retrained every 90 days
            expect(data.freshness_score).toBeGreaterThan(0.8);
        });
    });

    describe('Integration and End-to-End Tests', () => {
        test('should integrate all models in prediction pipeline', async () => {
            const result = await mockPythonExecution('integrated-pipeline.py', ['full_prediction']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.spending_prediction).toBeDefined();
            expect(data.anomaly_detection).toBeDefined();
            expect(data.risk_assessment).toBeDefined();
            expect(data.pipeline_latency_ms).toBeLessThan(500);
        });

        test('should handle model ensemble predictions', async () => {
            const result = await mockPythonExecution('ensemble-model.py', ['ensemble_predict']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.ensemble_prediction).toBeDefined();
            expect(data.individual_predictions).toBeDefined();
            expect(data.ensemble_confidence).toBeGreaterThan(0.8);
        });

        test('should validate model deployment readiness', async () => {
            const models = ['spending-prediction', 'anomaly-detection', 'risk-assessment'];
            
            for (const model of models) {
                const result = await mockPythonExecution(`${model}-model.py`, ['deployment_check']);
                const data = JSON.parse(result);
                
                expect(data.status).toBe('success');
                expect(data.deployment_ready).toBe(true);
                expect(data.health_check).toBe('passed');
                expect(data.dependencies_satisfied).toBe(true);
            }
        });

        test('should perform end-to-end user journey simulation', async () => {
            const result = await mockPythonExecution('e2e-simulation.py', ['user_journey']);
            const data = JSON.parse(result);

            expect(data.status).toBe('success');
            expect(data.journey_completed).toBe(true);
            expect(data.user_satisfaction_score).toBeGreaterThan(0.8);
            expect(data.system_reliability).toBeGreaterThan(0.95);
        });
    });
});

describe('Model Maintenance and Operations Tests', () => {
    test('should support model versioning and rollback', async () => {
        const result = await mockPythonExecution('model-ops.py', ['version_management']);
        const data = JSON.parse(result);

        expect(data.status).toBe('success');
        expect(data.current_version).toBeDefined();
        expect(data.rollback_capability).toBe(true);
        expect(data.version_history).toHaveLength.toBeGreaterThan(0);
    });

    test('should handle automated model retraining', async () => {
        const result = await mockPythonExecution('model-ops.py', ['auto_retrain']);
        const data = JSON.parse(result);

        expect(data.status).toBe('success');
        expect(data.retrain_triggered).toBeDefined();
        expect(data.retrain_schedule).toBeDefined();
    });

    test('should validate model backup and recovery', async () => {
        const result = await mockPythonExecution('model-ops.py', ['backup_recovery']);
        const data = JSON.parse(result);

        expect(data.status).toBe('success');
        expect(data.backup_created).toBe(true);
        expect(data.recovery_tested).toBe(true);
        expect(data.data_integrity_check).toBe('passed');
    });

    test('should monitor resource utilization', async () => {
        const result = await mockPythonExecution('model-ops.py', ['resource_monitoring']);
        const data = JSON.parse(result);

        expect(data.status).toBe('success');
        expect(data.cpu_utilization).toBeLessThan(80);
        expect(data.memory_utilization).toBeLessThan(85);
        expect(data.gpu_utilization).toBeLessThan(90);
    });
});